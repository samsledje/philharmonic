{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pk\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from networkx.linalg.graphmatrix import adjacency_matrix\n",
    "import hashlib\n",
    "##\n",
    "from clustering import read_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_name = 'SymbC1' # TODO: options: 'Pdam', 'SymbC1'\n",
    "\n",
    "BASE_DIR = './'\n",
    "DATA_DIR = f'{BASE_DIR}data/'\n",
    "OUTPUT_DIR = f'{BASE_DIR}results/'\n",
    "\n",
    "# POST_DSD_PARAMS:\n",
    "edge_weight_thresh = 0.5\n",
    "kclusts = None\n",
    "random_seed = 6191998\n",
    "\n",
    "# SPECTRAL CLUSTERING PARAMS:\n",
    "init_kclusts = 500\n",
    "cluster_divisor = 20 # yields max size of 30\n",
    "min_clust_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Parameters to Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming that the data has been downloaded from the google drive link, and net_name is either 'Pdam' or 'SymbC1', these do not need to be modified\n",
    "go_db_file =f\"{DATA_DIR}/go.obo\"\n",
    "\n",
    "interactionFile = f'{DATA_DIR}/{net_name}_predictions_positive.tsv'\n",
    "DSDfile = f'{DATA_DIR}/{net_name}_dscript_distances.DSD1.tsv' \n",
    "\n",
    "initial_SC_pickle_file = f\"{OUTPUT_DIR}{net_name}.initSC.pkl\"\n",
    "cluster_size_dist_file = f\"{OUTPUT_DIR}{net_name}.clust_size_dist.png\"\n",
    "initial_clusters_output_file = f\"{OUTPUT_DIR}{net_name}.clusters.csv\"\n",
    "\n",
    "post_recipe_clusters_filepath = f\"./{OUTPUT_DIR}{net_name}.recipe_clusters.csv\" # TODO: put this in relation to dir and netname\n",
    "\n",
    "\n",
    "print(f\"pre-recipe clusters will be saved to {initial_clusters_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post recipe writing clusters to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CLEAN THIS\n",
    "\n",
    "from pyvis.network import Network as PyvizNetwork\n",
    "COLAB_ENV = False\n",
    "\n",
    "class NetworkViz(PyvizNetwork):\n",
    "    \"\"\"Extend PyVis class so that we can use a modified template that works in Colab. \n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 height=\"500px\",\n",
    "                 width=\"500px\",\n",
    "                 directed=False,\n",
    "                 notebook=False,\n",
    "                 bgcolor=\"#ffffff\",\n",
    "                 font_color=False,\n",
    "                 layout=None,\n",
    "                 heading=\"\"):\n",
    "        # call super class init\n",
    "        PyvizNetwork.__init__(self, \n",
    "                              height, \n",
    "                              width,\n",
    "                              directed,\n",
    "                              notebook,\n",
    "                              bgcolor,\n",
    "                              font_color,\n",
    "                              layout,\n",
    "                              heading=heading)\n",
    "        # override template location - update as per installation\n",
    "        #self.path =  os.path.abspath(os.path.join(os.path.dirname( __file__ ), '..', 'templates/pyvis_inline.html'))\n",
    "\n",
    "\n",
    "    # fun copied from pyvis to skip imports\n",
    "    def check_html(self, name):\n",
    "        \"\"\"\n",
    "        Given a name of graph to save or write, check if it is of valid syntax\n",
    "        :param: name: the name to check\n",
    "        :type name: str\n",
    "        \"\"\"\n",
    "        assert len(name.split(\".\")) >= 2, \"invalid file type for %s\" % name\n",
    "        assert name.split(\n",
    "            \".\")[-1] == \"html\", \"%s is not a valid html file\" % name\n",
    "\n",
    "    # fun extended for colab\n",
    "    def show(self, name):\n",
    "        \"\"\"\n",
    "        Writes a static HTML file and saves it locally before opening.\n",
    "        :param: name: the name of the html file to save as\n",
    "        :type name: str\n",
    "        \"\"\"\n",
    "        self.check_html(name)\n",
    "        if self.template is not None:\n",
    "            if not COLAB_ENV: \n",
    "                # write file and return IFrame\n",
    "                return self.write_html(name, notebook=True)\n",
    "            else:\n",
    "                # write file and return HTML\n",
    "                self.write_html(name, notebook=True)\n",
    "                return IPython.display.HTML(data=name)\n",
    "        else:\n",
    "            self.write_html(name)\n",
    "            webbrowser.open(name)\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,proteins):\n",
    "        self.proteins = proteins\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proteins)\n",
    "\n",
    "    def __repr__(self): # NOTE: \n",
    "        reprStr = \"Cluster of {} [{},{},...] (hash {})\".format(len(self), self.proteins[0], self.proteins[1], hash(self))\n",
    "        if hasattr(self, 'G'):\n",
    "            reprStr += \"\\nTriangles: {}\\nMax Degree: {}\".format(self.triangles(), max(self.G.degree(), key=lambda x: x[1])[1])\n",
    "        if hasattr(self, 'GO_terms'):\n",
    "            reprStr += \"\\nTop Terms:\\n\\t{}\".format('\\n\\t'.join(\n",
    "                    ['{} ({})'.format(i[0], i[1]) for i in self.get_top_terms(5)]\n",
    "            ))\n",
    "        return reprStr\n",
    "\n",
    "    def __hash__(self):\n",
    "        return int(hashlib.md5(''.join(self.proteins).encode()).hexdigest(), 16)\n",
    "    \n",
    "    def __iter_(self):\n",
    "        return iter(self.proteins)\n",
    "\n",
    "    def to_dict(self): # NOTE: \n",
    "        D = {}\n",
    "        D['id'] = hash(self)\n",
    "        D['proteins'] = []\n",
    "        for p in self.proteins:\n",
    "            pD = {}\n",
    "            pD['name'] = p\n",
    "            if hasattr(self, 'GO_DB'):\n",
    "                pD['go'] = self.GO_DB[self.GO_DB['seq'] == p]['GO_ids'].values[0]\n",
    "            D['proteins'].append(pD)\n",
    "        if hasattr(self, 'GO_DB'):\n",
    "            D['go'] = sorted([{\"id\": i.ID, \"desc\": i.name, \"freq\": self.GO_terms[i]} for i in self.GO_terms], key = lambda x: x['freq'], reverse=True)\n",
    "        if hasattr(self,'G'):\n",
    "            D['graph'] = list(self.G.edges())\n",
    "        return D\n",
    "\n",
    "    def to_json(self): # NOTE:\n",
    "        return json.dumps(self.to_dict())\n",
    "\n",
    "    def add_GO_terms(self, go_db, GO_OBJECTS):\n",
    "        self.GO_terms = {}\n",
    "        self.GO_DB = go_db\n",
    "        for prot in self.proteins:\n",
    "            goIds = go_db[go_db['seq'] == prot]['GO_ids'].values[0]\n",
    "            if goIds is None or len(goIds) == 0:\n",
    "                continue\n",
    "            for gid in goIds:\n",
    "                try:\n",
    "                    goObj = GO_OBJECTS[gid]\n",
    "                except KeyError:\n",
    "                    GO_OBJECTS[gid] = GO(gid,{'id':gid,'name':gid})\n",
    "                    goObj = GO_OBJECTS[gid]\n",
    "                goCount = self.GO_terms.setdefault(goObj,0)\n",
    "                self.GO_terms[goObj] = goCount + 1\n",
    "\n",
    "    def get_proteins_by_GO(self, GO_id):\n",
    "        return [p for p in self.proteins if GO_id in prot_go_db.loc[p,'GO_ids']]\n",
    "\n",
    "    def get_GO_by_protein(self, protein):\n",
    "        assert protein in self.proteins, \"{} not in cluster\".format(protein)\n",
    "        return [gt for gt in coi.GO_terms if gt.ID in prot_go_db.loc[protein,'GO_ids']]\n",
    "\n",
    "    def get_top_terms(self,N):\n",
    "        if not hasattr(self, 'GO_terms'):\n",
    "            raise NotImplementedError(\"GO Terms have not been added yet.\")\n",
    "        GOlist = list(self.GO_terms.keys())\n",
    "        if N == -1:\n",
    "            N = len(GOlist)\n",
    "        sortedList = sorted(GOlist,key=lambda x: self.GO_terms[x],reverse=True)[:N]\n",
    "        return list(zip(sortedList, [self.GO_terms[i] for i in sortedList]))\n",
    "\n",
    "    def set_graph(self,G):\n",
    "        self.G = G.subgraph(self.proteins)\n",
    "\n",
    "    def triangles(self):\n",
    "        return sum([i for i in nx.triangles(self.G).values()]) / 3\n",
    "\n",
    "    def draw_degree_histogram(self,draw_graph=True):\n",
    "        if not hasattr(self,'G'):\n",
    "            raise ValueError('Run .set_graph() method on this cluster first')\n",
    "        G = self.G\n",
    "        degree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\n",
    "        degreeCount = collections.Counter(degree_sequence)\n",
    "        deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(deg, cnt, width=0.80, color='b')\n",
    "\n",
    "        plt.title(\"Degree Histogram\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "        ax.set_xticks([d + 0.4 for d in deg])\n",
    "        ax.set_xticklabels(deg)\n",
    "\n",
    "        # draw graph in inset\n",
    "        if draw_graph:\n",
    "            plt.axes([0.4, 0.4, 0.5, 0.5])\n",
    "            pos = nx.spring_layout(G, k=0.15,iterations=10)\n",
    "            plt.axis('off')\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=20)\n",
    "            nx.draw_networkx_edges(G, pos, alpha=0.4)\n",
    "        plt.show()\n",
    "\n",
    "    def draw_graph(self, buttons=False):\n",
    "        if not hasattr(self,'G'):\n",
    "            raise ValueError('Run .set_graph() method on this cluster first')\n",
    "        net = NetworkViz(width=\"500px\", height=\"500px\", notebook=True)\n",
    "        net.from_nx(self.G)\n",
    "        if buttons: net.show_buttons()\n",
    "        return net.show(f\"{hash(self)}_graph.html\")\n",
    "        #nx.draw_kamada_kawai(G, with_labels=True,node_size=600, font_size=8)\n",
    "    \n",
    "def readClusterObjects(infile,sep=','):\n",
    "    clusts = []\n",
    "    with open(infile,'r') as f:\n",
    "        for line in f:\n",
    "            clusts.append(Cluster(line.strip().split(sep)))\n",
    "    return clusts\n",
    "\n",
    "def cluster_from_json(jsonString, GO_OBJECTS):\n",
    "        clust = Cluster([])\n",
    "        D = json.loads(jsonString)\n",
    "        clust.proteins = [i['name'] for i in D['proteins']]\n",
    "        clust.GO_terms = {}\n",
    "        for goDict in D['go']:\n",
    "            gid = goDict['id']\n",
    "            gdesc = goDict['desc']\n",
    "            try:\n",
    "                goObj = GO_OBJECTS[gid]\n",
    "            except KeyError:\n",
    "                GO_OBJECTS[gid] = GO(gid,{'id':gid,'name':gdesc})\n",
    "                goObj = GO_OBJECTS[gid]\n",
    "            clust.GO_terms[goObj] = goDict['freq']\n",
    "        try:\n",
    "            edgeList = D['graph']\n",
    "            G = nx.Graph()\n",
    "            for e in edgeList:\n",
    "                G.add_edge(*e)\n",
    "            clust.G = G\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return clust\n",
    "\n",
    "class GO:\n",
    "    def __init__(self, ID, features):\n",
    "        self.ID = ID\n",
    "        self.D = features\n",
    "        self.name = features['name']\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{} - <{}>'.format(self.ID, self.name)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.ID == other.ID\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.ID)\n",
    "\n",
    "def extract_GO_id_from_list(l):\n",
    "    if isinstance(l,list):\n",
    "        return [i.split('|')[0] for i in l]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def read_GO_obo(infile): # TODO: throw errors \n",
    "    terms = {}\n",
    "    with open(infile,'r') as f:\n",
    "        for line in f:\n",
    "            tDict = {}\n",
    "            line = line.strip()\n",
    "            if line == \"[Term]\":\n",
    "                line = f.readline().strip().split(': ')\n",
    "                while not line == ['']:\n",
    "                    tDict[line[0]] = ''.join(line[1:])\n",
    "                    line = f.readline().strip().split(': ')\n",
    "                for k,v in tDict.items():\n",
    "                    k = k.strip()\n",
    "                    v = v.strip()\n",
    "                    tDict[k] = v\n",
    "                terms[tDict['id']] = GO(tDict['id'], tDict)\n",
    "    return terms\n",
    "    \n",
    "def triangle_search(clusters, min_triangles=0, max_triangles=np.infty):\n",
    "    return [c for c in clusters if c.triangles() >= min_triangles and c.triangles() <= max_triangles]\n",
    "\n",
    "def node_search(clusters, min_nodes=0, max_nodes=np.infty):\n",
    "    return [c for c in clusters if len(c) >= min_nodes and len(c) <= max_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"reading clusters from {post_recipe_clusters_filepath}\")\n",
    "clusters = readClusterObjects(post_recipe_clusters_filepath)\n",
    "\n",
    "G, DSD, protein_names = read_network(DSDfile, interactionFile, edge_weight_thresh=edge_weight_thresh, net_name=net_name, output_stats=True, results_dir=OUTPUT_DIR)\n",
    "fullNetwork = Cluster(protein_names) \n",
    "fullNetwork.set_graph(G)\n",
    "for i in clusters:\n",
    "    i.set_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_GO_map(f):\n",
    "    seqDb = pd.read_csv(f,sep=',')\n",
    "    seqDb.columns = ['seq','manual_annot','pfam_list','GO_list']\n",
    "    seqDb['GO_str'] = seqDb['GO_list']\n",
    "    seqDb['GO_list'] = seqDb['GO_str'].str.split(';')\n",
    "    def extract_GO_id_from_list(l):\n",
    "        if isinstance(l,list):\n",
    "            return [i.split('|')[0] for i in l]\n",
    "        else:\n",
    "            return None\n",
    "    seqDb['GO_ids'] = seqDb['GO_list'].apply(extract_GO_id_from_list)\n",
    "    seq2GO = seqDb[['seq','GO_ids']]\n",
    "    seq2GO.columns = ['seq','GO_ids']\n",
    "    return seq2GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdamMap = clean_GO_map(f'{DATA_DIR}Pdam_GO_map.csv') # TODO: should be able to generate, but will have to figure out rohit's process \n",
    "symbMap = clean_GO_map(f'{DATA_DIR}SymbC1_GO_map.csv')\n",
    "goMap = pd.concat((pdamMap, symbMap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_OBJECTS = read_GO_obo(go_db_file)\n",
    "print('Adding GO Annotations...')\n",
    "for clust in tqdm(clusters):\n",
    "    clust.add_GO_terms(goMap,GO_OBJECTS)\n",
    "fullNetwork.add_GO_terms(goMap,GO_OBJECTS)\n",
    "clusters.sort(key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND NOW THE CLUSTERS ARE DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT TO DO WITH CLUSTERS:\n",
    "- print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is added to be able to print the clusters to files for lenore\n",
    "summary_output_filename = f'{BASE_DIR}{net_name}_pretty_clusters_summary.txt'\n",
    "with (open(summary_output_filename, 'w+')) as f:\n",
    "    for i in clusters:\n",
    "        f.write(i.__repr__() + '\\n')\n",
    "    # f.write(json.dumps([i.to_dict() for i in clusters]))\n",
    "print(f\"pretty summary printed to {summary_output_filename}\")\n",
    "hash_to_proteins_file = f'{BASE_DIR}{net_name}_hash_to_proteins.json'\n",
    "with (open(hash_to_proteins_file, 'w+')) as f:\n",
    "    for i in clusters:\n",
    "        f.write(json.dumps({hash(i): i.proteins}) + '\\n')\n",
    "print(f\"cluster info printed to {hash_to_proteins_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analysis of clusters (how many times were proteins added back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph\n",
    "\n",
    "\n",
    "import collections\n",
    "recipe_json = f\"./results/{net_name}.recipe_clusters.json\"\n",
    "\n",
    "# determine distribution of protein re-addition.\n",
    "recipe_prots = {}\n",
    "with open(recipe_json) as f:\n",
    "    recipe_prots = json.load(f)\n",
    "\n",
    "# create a default dict where the key is a protein and the initial val is 0\n",
    "# then for each cluster, increment the count for each protein in the cluster\n",
    "prot_counts = collections.defaultdict(int)\n",
    "for k in recipe_prots[\"degree\"][\"0.75\"].keys():\n",
    "    for prot in recipe_prots[\"degree\"][\"0.75\"][k]:\n",
    "        prot_counts[prot] += 1\n",
    "\n",
    "# plot the distribution of proteins added\n",
    "plt.hist(prot_counts.values(), bins=range(1, max(prot_counts.values())+1))\n",
    "plt.xlabel('Number of times protein was added')\n",
    "plt.ylabel('Number of proteins')\n",
    "\n",
    "# make y axis log scale\n",
    "plt.yscale('log')\n",
    "# add count on the top of the histogram bar\n",
    "\n",
    "plt.title(f'Distribution of proteins added ({net_name})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
